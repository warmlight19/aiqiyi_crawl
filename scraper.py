import os
import re
import time
import json
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import logging
import requests

# åˆ›å»ºçº¿ç¨‹é”
file_lock = threading.Lock()
print_lock = threading.Lock()

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
from datetime import datetime

def setup_driver(headless=False):
    """
    é…ç½®å¹¶å¯åŠ¨ Chrome æµè§ˆå™¨é©±åŠ¨
    :return: æµè§ˆå™¨é©±åŠ¨å¯¹è±¡
    """
    chrome_options = Options()
    chrome_options.add_argument("--ignore-certificate-errors")
    if headless:
        chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--log-level=3")  # 3 è¡¨ç¤ºåªæ˜¾ç¤º FATAL çº§åˆ«çš„æ—¥å¿—
    # chrome_options.add_argument('--disable-gpu')  # ç¦ç”¨GPUç¡¬ä»¶åŠ é€Ÿ
    # chrome_options.add_argument('--disable-software-rasterizer')  # ç¦ç”¨è½¯ä»¶æ¸²æŸ“åå¤‡
    # chrome_options.add_argument('--use-angle=swiftshader')  # å¼ºåˆ¶ä½¿ç”¨ SwiftShader è½¯ä»¶æ¸²æŸ“
    # chrome_options.add_argument('--disable-features=Vulkan')  

    chrome_driver_path = 'D:\\python3.10.3\\chromedriver.exe'
    service = Service(chrome_driver_path)
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.maximize_window()
    return driver

def outo_login(driver):
    """
    æ·»åŠ Cookieåˆ°å½“å‰ä¼šè¯ï¼Œå®ç°ç™»å½•
    """
    with open('iqiyi_cookies.json', 'r') as f:
        cookies = json.load(f)

    for cookie in cookies:
        driver.add_cookie(cookie)
        
    driver.refresh()

def parse_csv(file_path):
    try:
        # è¯»å–æ—¶æŒ‡å®šé”™è¯¯å¤„ç†å›è°ƒ
        def parse_error_handler(error):
            logging.error(f"è§£æé”™è¯¯å‘ç”Ÿåœ¨è¡Œ {error.row}: {str(error)}")
            logging.error(f"é”™è¯¯è¡Œå†…å®¹ï¼š{error.line}")
            return

        df = pd.read_csv(file_path, on_bad_lines=parse_error_handler)
        
        # å°è¯•ååºåˆ—åŒ–è¯„è®ºå­—æ®µ
        if 'comments' in df.columns:
            def safe_json_loads(x):
                if pd.isna(x):
                    return []
                try:
                    # é¢„å¤„ç†éæ³•å­—ç¬¦ï¼ˆåŒ…å«æ›´å¤šç‰¹æ®Šå­—ç¬¦å¤„ç†ï¼‰
                    x = re.sub(r'[\x00-\x1F\x7F]', '', x)
                    x = x.replace('\\', '/')
                    decoder = json.JSONDecoder(strict=False)
                    return decoder.decode(x)
                except json.JSONDecodeError as e:
                    logging.error(f"è¯„è®ºæ•°æ®è§£æå¤±è´¥ï¼ŒåŸå§‹å†…å®¹é•¿åº¦ï¼š{len(x)}ï¼Œæˆªå–ç‰‡æ®µï¼š{x[max(0,e.pos-50):e.pos+50]}")
                    return []
                except Exception as e:
                    logging.error(f"æœªçŸ¥è§£æé”™è¯¯ï¼š{str(e)}")
                    return []

            df['comments'] = df['comments'].apply(safe_json_loads)
        
        return df
    except pd.errors.EmptyDataError:
        logging.warning(f'æ–‡ä»¶ {file_path} ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼å¡«å……')
        return pd.DataFrame()
    except pd.errors.ParserError as e:
        logging.error(f'è§£ææ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}')
        return pd.DataFrame()
    except Exception as e:
        logging.error(f'å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}', exc_info=True)
        return pd.DataFrame()


def click_hottest_tab(driver):
    """
    ç‚¹å‡»çˆ±å¥‡è‰ºé¦–é¡µçš„"æœ€çƒ­"æ ‡ç­¾
    :param driver: æµè§ˆå™¨é©±åŠ¨å¯¹è±¡
    """
    try:
        # ç­‰å¾…æœ€çƒ­æ ‡ç­¾å‡ºç°
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//div[@data-role="button" and contains(.//span, "æœ€çƒ­")]'))
        )
        hottest_tab = driver.find_element(By.XPATH, '//div[@data-role="button" and contains(.//span, "æœ€çƒ­")]')
        hottest_tab.click()
        # ç­‰å¾…ç‚¹å‡»åå†…å®¹åŠ è½½
        time.sleep(3)
    except Exception as e:
        print(f"æ— æ³•æ‰¾åˆ°æˆ–ç‚¹å‡»æœ€çƒ­æ ‡ç­¾ï¼š{e}")
        time.sleep(2)
    time.sleep(1)

def get_video_list(driver, video_count=10): ######################################
    """
    è·å–"æœ€çƒ­"æ ‡ç­¾ä¸‹çš„è§†é¢‘åˆ—è¡¨
    :param driver: æµè§ˆå™¨é©±åŠ¨å¯¹è±¡
    :param video_count: è¦çˆ¬å–çš„è§†é¢‘æ•°é‡
    :return: è§†é¢‘åˆ—è¡¨ï¼ŒåŒ…å«è§†é¢‘åç§°å’Œé“¾æ¥
    """
    page_source = driver.page_source
    soup = BeautifulSoup(page_source, 'html.parser')

    # æŸ¥æ‰¾è§†é¢‘å¡ç‰‡
    video_cards = soup.find_all('div', class_='flex-video-list_flexTilesItem__bhNS9')
    videos = []

    for card in video_cards[:video_count]:
        # æå–è§†é¢‘åç§°
        video_name_tag = card.find('p', class_='undefined flex-video-list_title__pTUUg')
        video_name = video_name_tag.get_text(strip=True) if video_name_tag else None
        
        # æå–è§†é¢‘é“¾æ¥
        parent_link = card.find('a')
        video_link = parent_link['href'] if parent_link and parent_link.get('href') else None
        
        # æå–å°é¢å›¾ç‰‡é“¾æ¥
        poster_img = card.find('img', class_='flex-video-list_poster__cNMg+')
        poster_url = poster_img['src'] if poster_img and poster_img.get('src') else "static/default-poster.svg"
        if not poster_url:
            print(f"æœªæ‰¾åˆ° {video_name} çš„å°é¢å›¾ç‰‡é“¾æ¥")
        
        # å°†è§†é¢‘ä¿¡æ¯ä¿å­˜åˆ°åˆ—è¡¨
        if video_name and video_link and poster_url:
            videos.append({
                "name": video_name,
                "link": video_link,
                "poster_url": poster_url  # æ–°å¢å°é¢å›¾ç‰‡é“¾æ¥å­—æ®µ
            })

    return videos

def get_video_info(driver, video_name, video_link, poster_url, is_recommendation=False):
    """
    è·å–å•ä¸ªè§†é¢‘çš„åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…æ‹¬è¯„åˆ†æ¬¡æ•°ã€ç®€ä»‹ã€æ¼”å‘˜è¡¨
    :param driver: æµè§ˆå™¨é©±åŠ¨å¯¹è±¡
    :param video_name: è§†é¢‘åç§°
    :param video_link: è§†é¢‘é“¾æ¥
    :return: è§†é¢‘åŸºæœ¬ä¿¡æ¯
    """
    driver.get(video_link)
    # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
    except Exception as e:
        print(f"ç­‰å¾…é¡µé¢åŠ è½½è¶…æ—¶: {e}")
    time.sleep(3)  # é¢å¤–ç­‰å¾…ç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½å®Œæˆ

    if not is_recommendation:
        # å¦‚æœä¸æ˜¯æ¨èè§†é¢‘ï¼Œå…ˆè·å–æ¨èè§†é¢‘ä¿¡æ¯
        recommendations = get_recommended_videos(driver)
        if recommendations:
            save_recommendations_to_csv(video_name, recommendations)
    
    driver.get(video_link)
    # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
    except Exception as e:
        print(f"ç­‰å¾…é¡µé¢åŠ è½½è¶…æ—¶: {e}")
    time.sleep(3)  # é¢å¤–ç­‰å¾…ç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½å®Œæˆ

    try:
        detail_button = driver.find_element(By.XPATH, '//div[@class="meta_arrowBtn__gw1b5 meta_detailBtn__2XYRK"]')
        detail_button.click()
    except Exception as e:
        print(f"æ— æ³•ç‚¹å‡»è¯¦æƒ…æŒ‰é’®ï¼š{e}")
        return None
    time.sleep(1)

    detail_page_source = driver.page_source
    detail_soup = BeautifulSoup(detail_page_source, 'html.parser')

    # è®¾ç½®è¯„åˆ†å’Œè¯„åˆ†äººæ•°çš„é»˜è®¤å€¼
    rating = detail_soup.find('div', class_='score_scoreLabel__fYRiV')
    rating = rating.get_text(strip=True) if rating and rating.get_text(strip=True) else "æš‚æ— æ¨èåˆ†\næ•¬è¯·æœŸå¾…"
    
    rating_count = detail_soup.find('div', class_='score_scoreCount__aNzvS')
    rating_count = rating_count.get_text(strip=True) if rating_count else "æš‚æ— è¯„ä»·"
    
    # è®¾ç½®ç®€ä»‹çš„é»˜è®¤å€¼
    description = detail_soup.find('div', class_='metaDetail_infoValue__NNS+T')
    description = description.get_text(strip=True) if description else "æš‚æ— ç®€ä»‹"
    
    # è®¾ç½®æ¼”å‘˜è¡¨çš„é»˜è®¤å€¼
    cast_list = detail_soup.find_all('div', class_='star-list_name__VSd6I')
    cast = ", ".join([cast.get_text(strip=True) for cast in cast_list]) if cast_list else "æš‚æ— æ¼”èŒäººå‘˜ä¿¡æ¯"

    # # è·å–æ¨èè§†é¢‘ä¿¡æ¯
    # recommendations = get_recommended_videos(driver)
    # if recommendations:
    #     save_recommendations_to_csv(video_name, recommendations)

    video_info = {
        "name": video_name,
        "link": video_link,
        "poster_url": poster_url,
        "rating": rating,
        "rating_count": rating_count,
        "description": description,
        "cast": cast
    }
    return video_info

def get_recommended_videos(driver):
    """
    è·å–å½“å‰è§†é¢‘é¡µé¢çš„æ¨èè§†é¢‘åˆ—è¡¨å¹¶è·å–æ¯ä¸ªæ¨èè§†é¢‘çš„è¯¦ç»†ä¿¡æ¯
    :param driver: æµè§ˆå™¨é©±åŠ¨å¯¹è±¡
    :return: æ¨èè§†é¢‘åˆ—è¡¨ï¼ˆåŒ…å«è¯¦ç»†ä¿¡æ¯ï¼‰
    """
    try:
        print("\nå¼€å§‹è·å–æ¨èè§†é¢‘åˆ—è¡¨...")
        
        # ç­‰å¾…æ¨èåŒºåŸŸåŠ è½½å®Œæˆ
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.ID, "recommend_bk"))
        )
        
        # è·å–é¡µé¢æºç 
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # æŸ¥æ‰¾æ¨èè§†é¢‘åˆ—è¡¨
        recommended_videos = []
        basic_video_info = []
        
        # 1. é¦–å…ˆå®šä½æ¨èå®¹å™¨
        recommend_container = soup.find('div', id='recommend_bk')
        if not recommend_container:
            print("âŒ æœªæ‰¾åˆ°æ¨èè§†é¢‘å®¹å™¨")
            return []
            
        # 2. æŸ¥æ‰¾æ‰€æœ‰æ¨èè§†é¢‘å¡ç‰‡å®¹å™¨
        video_cards = recommend_container.find_all('div', style=lambda x: x and 'width: 162px; height: 266px;' in x)
        print(f"ğŸ“Š æ‰¾åˆ° {len(video_cards)} ä¸ªæ¨èè§†é¢‘å¡ç‰‡")

        # 3. éå†å¡ç‰‡ç›´åˆ°è·å–10ä¸ªæœ‰æ•ˆè§†é¢‘çš„åŸºæœ¬ä¿¡æ¯
        valid_count = 0
        for i, card in enumerate(video_cards):
            if valid_count >= 10: ######################
                break
            
            print(f"\nğŸ” æ­£åœ¨å¤„ç†ç¬¬ {i+1} ä¸ªæ¨èè§†é¢‘åŸºæœ¬ä¿¡æ¯...")
            
            try:
                # --- 1. è·å–åŸºæœ¬æ•°æ® ---
                # è§†é¢‘é“¾æ¥ï¼ˆä»å›¾ç‰‡æˆ–æ ‡é¢˜åŒºåŸŸè·å–ï¼‰
                video_link = None
                for link in card.find_all('a', href=True):
                    if link['href'].startswith(('http', '//', '/')):
                        video_link = link['href']
                        if video_link.startswith('//'):
                            video_link = 'https:' + video_link
                        elif video_link.startswith('/'):
                            video_link = 'https://www.iqiyi.com' + video_link
                        break
                
                # è§†é¢‘åç§°
                title_span = card.find('span', class_=lambda x: x and 'title__' in x)
                video_name = title_span.get_text(strip=True) if title_span else None
                
                # å°é¢å›¾ç‰‡
                poster_img = card.find('img', class_=lambda x: x and 'videoImage' in x)
                poster_url = (poster_img['src'] if poster_img and poster_img.get('src') 
                             else "static/default-poster.svg")
                
                # --- 2. æ•°æ®éªŒè¯ ---
                if not all([video_link, video_name]):
                    print(f"âš ï¸ è·³è¿‡æ— æ•ˆå¡ç‰‡ï¼šç¼ºå°‘å¿…è¦æ•°æ® | åç§°:{video_name} | é“¾æ¥:{video_link}")
                    continue
                
                # --- 3. è·å–æ‰©å±•ä¿¡æ¯ ---
                # è§†é¢‘ç±»å‹å’Œé›†æ•°
                type_tag = card.find('div', class_=lambda x: x and 'TypeTag' in x)
                episode_tag = card.find('div', class_=lambda x: x and 'subscript' in x)
                
                # --- 4. æ„å»ºåŸºæœ¬è§†é¢‘æ•°æ® ---
                video_data = {
                    "name": video_name,
                    "link": video_link,
                    "poster_url": poster_url,
                    "type": type_tag.get_text(strip=True) if type_tag else None,
                    "episodes": episode_tag.get_text(strip=True) if episode_tag else None
                }
                
                # æ·»åŠ åˆ°åŸºæœ¬ä¿¡æ¯åˆ—è¡¨
                basic_video_info.append(video_data)
                valid_count += 1
                print(f"âœ… æˆåŠŸè·å–ç¬¬ {valid_count} ä¸ªè§†é¢‘åŸºæœ¬ä¿¡æ¯: {video_name}")
                
            except Exception as e:
                print(f"âŒ å¤„ç†å¡ç‰‡åŸºæœ¬ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
                continue
        
        # å¦‚æœä¸è¶³10ä¸ªï¼Œå°è¯•æ»šåŠ¨åŠ è½½æ›´å¤š
        scroll_attempts = 0
        while len(basic_video_info) < 10 and scroll_attempts < 3: ###################
            print(f"\nğŸ”„ éœ€è¦æ›´å¤šæœ‰æ•ˆè§†é¢‘ï¼Œå°è¯•æ»šåŠ¨åŠ è½½ (å°è¯• {scroll_attempts+1}/3)...")
            driver.execute_script("window.scrollBy(0, 800);")
            time.sleep(2)
            
            # é‡æ–°è·å–å¡ç‰‡
            page_source = driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            new_cards = soup.find('div', id='recommend_bk').find_all(
                'div', style=lambda x: x and 'width: 162px; height: 266px;' in x)
            
            # å¤„ç†æ–°å¢å¡ç‰‡çš„åŸºæœ¬ä¿¡æ¯
            for card in new_cards[len(video_cards):]:
                if len(basic_video_info) >= 10:            #############
                    break
                    
                try:
                    # è·å–è§†é¢‘é“¾æ¥
                    video_link = None
                    for link in card.find_all('a', href=True):
                        if link['href'].startswith(('http', '//', '/')):
                            video_link = link['href']
                            if video_link.startswith('//'):
                                video_link = 'https:' + video_link
                            elif video_link.startswith('/'):
                                video_link = 'https://www.iqiyi.com' + video_link
                            break
                    
                    # è·å–è§†é¢‘åç§°
                    title_span = card.find('span', class_=lambda x: x and 'title__' in x)
                    video_name = title_span.get_text(strip=True) if title_span else None
                    
                    # è·å–å°é¢å›¾ç‰‡
                    poster_img = card.find('img', class_=lambda x: x and 'videoImage' in x)
                    poster_url = (poster_img['src'] if poster_img and poster_img.get('src') 
                                 else "static/default-poster.svg")
                    
                    if not all([video_link, video_name]):
                        continue
                    
                    # è·å–ç±»å‹å’Œé›†æ•°ä¿¡æ¯
                    type_tag = card.find('div', class_=lambda x: x and 'TypeTag' in x)
                    episode_tag = card.find('div', class_=lambda x: x and 'subscript' in x)
                    
                    video_data = {
                        "name": video_name,
                        "link": video_link,
                        "poster_url": poster_url,
                        "type": type_tag.get_text(strip=True) if type_tag else None,
                        "episodes": episode_tag.get_text(strip=True) if episode_tag else None
                    }
                    
                    basic_video_info.append(video_data)
                    valid_count += 1
                    print(f"âœ… æˆåŠŸè·å–ç¬¬ {valid_count} ä¸ªè§†é¢‘åŸºæœ¬ä¿¡æ¯: {video_name}")
                    
                except Exception as e:
                    print(f"âŒ å¤„ç†æ–°å¢å¡ç‰‡æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            video_cards = new_cards
            scroll_attempts += 1

        print(f"\nğŸ¯ æˆåŠŸè·å– {len(basic_video_info)} ä¸ªè§†é¢‘çš„åŸºæœ¬ä¿¡æ¯")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè·å–è¯¦ç»†ä¿¡æ¯
        with ThreadPoolExecutor(max_workers=6) as executor:
            def get_video_details(video_data):
                try:
                    # åˆ›å»ºæ–°çš„æµè§ˆå™¨å®ä¾‹
                    detail_driver = setup_driver()
                    try:
                        # è·å–è¯¦ç»†ä¿¡æ¯
                        video_info = get_video_info(detail_driver, video_data["name"], 
                                                  video_data["link"], video_data["poster_url"], 
                                                  is_recommendation=True)
                        if video_info:
                            video_data.update({
                                "rating": video_info.get("rating"),
                                "rating_count": video_info.get("rating_count"),
                                "description": video_info.get("description"),
                                "cast": video_info.get("cast")
                            })
                        
                        # è·å–è¯„è®º
                        comments = get_video_comments(detail_driver, video_data["name"], video_data["link"])
                        video_data["comments"] = comments
                        
                        with print_lock:
                            print(f"âœ… æˆåŠŸè·å– {video_data['name']} çš„è¯¦ç»†ä¿¡æ¯")
                        
                        return video_data
                    finally:
                        detail_driver.quit()
                except Exception as e:
                    with print_lock:
                        print(f"âš ï¸ è·å– {video_data['name']} è¯¦ç»†ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
                    return video_data
            
            # æäº¤æ‰€æœ‰ä»»åŠ¡å¹¶ç­‰å¾…å®Œæˆ
            future_to_video = {executor.submit(get_video_details, video): video 
                              for video in basic_video_info[:10]}         #######################
            
            for future in as_completed(future_to_video):
                try:
                    video_data = future.result()
                    if video_data:
                        recommended_videos.append(video_data)
                except Exception as e:
                    print(f"âŒ å¤„ç†è§†é¢‘è¯¦ç»†ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        
        print(f"\nğŸ‰ å®Œæˆæ‰€æœ‰è§†é¢‘ä¿¡æ¯è·å–ï¼Œå…± {len(recommended_videos)} ä¸ªè§†é¢‘")
        return recommended_videos
        
    except Exception as e:
        print(f"âŒ è·å–æ¨èè§†é¢‘åˆ—è¡¨æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
        return []

def save_recommendations_to_csv(video_name, recommendations):
    """
    å°†æ¨èè§†é¢‘ä¿¡æ¯ä¿å­˜åˆ°CSVæ–‡ä»¶
    :param video_name: ä¸»è§†é¢‘åç§°
    :param recommendations: æ¨èè§†é¢‘åˆ—è¡¨
    """
    try:
        with file_lock:
            # åºåˆ—åŒ–è¯„è®ºæ•°æ®
            for video in recommendations:
                if 'comments' in video:
                    try:
                        video['comments'] = json.dumps(video['comments'], ensure_ascii=False)
                    except Exception as e:
                        logging.error(f"åºåˆ—åŒ–å¤±è´¥è¯„è®ºæ•°æ®ï¼š{video['comments']}")
                        raise

            recommendations_df = pd.DataFrame(recommendations)
            
            # è·å–å½“å‰æ—¶é—´å¹¶æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
            current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # æŒ‡å®šç›¸å¯¹è·¯å¾„ï¼Œæ–‡ä»¶åä¸­åŒ…å«å½“å‰æ—¶é—´
            save_path = os.path.join("data/recommendations", f"{video_name}_{current_time}.csv")
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # ä¿å­˜ DataFrame åˆ° CSV æ–‡ä»¶
            recommendations_df.to_csv(save_path, index=False, encoding="utf-8-sig")
            logging.info(f"æ¨èè§†é¢‘ä¿¡æ¯å·²ä¿å­˜åˆ° {save_path} æ–‡ä»¶ä¸­ã€‚")
    except Exception as e:
        logging.error(f"ä¿å­˜æ¨èè§†é¢‘CSVæ–‡ä»¶æ—¶å‡ºé”™ï¼š{e}", exc_info=True)
        return False
    return True

def get_video_comments(driver, video_name, video_link):
    """
    è·å–è§†é¢‘çš„è¯„è®ºä¿¡æ¯
    :param driver: æµè§ˆå™¨é©±åŠ¨å¯¹è±¡
    :param video_name: è§†é¢‘åç§°
    :param video_link: è§†é¢‘é“¾æ¥
    :return: è§†é¢‘è¯„è®ºä¿¡æ¯åˆ—è¡¨
    """
    driver.get(video_link)
    # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
    except Exception as e:
        print(f"ç­‰å¾…é¡µé¢åŠ è½½è¶…æ—¶: {e}")
    time.sleep(3)  # é¢å¤–ç­‰å¾…ç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½å®Œæˆ

    try:
        discussion_tab = driver.find_element(By.XPATH, '//div[@class="tab-menu_tabItem__VJQDn " and contains(.//div[@class="tab-menu_title__k8gOR"], "è®¨è®º")]')
        discussion_tab.click()
        print(f"æˆåŠŸåˆ‡æ¢åˆ°è®¨è®ºæ ‡ç­¾é¡µ")
        # ç­‰å¾…è®¨è®ºå†…å®¹åŠ è½½
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'div[id^="comment"]'))
            )
        except Exception as e:
            print(f"ç­‰å¾…è®¨è®ºå†…å®¹åŠ è½½è¶…æ—¶: {e}")
    except Exception as e:
        print(f"æ— æ³•ç‚¹å‡»è®¨è®ºæ ‡ç­¾ï¼š{e}")
        return [{"nickname": "", "user_id": "", "avatar_url": "", "comment_time": "", "comment_content": "", "like_count": ""}]
    time.sleep(3)  # é¢å¤–ç­‰å¾…ç¡®ä¿è¯„è®ºå†…å®¹å®Œå…¨åŠ è½½

    discussion_page_source = driver.page_source
    discussion_soup = BeautifulSoup(discussion_page_source, 'html.parser')

    comment_divs = discussion_soup.select('div[id^="comment"]')[:10]

    comments = []
    for index, comment_div in enumerate(comment_divs):
        try:
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å±•å¼€æŒ‰é’®
            expand_button = comment_div.find('div', class_='collapse-text_expandBtn__R7Ga3 comments_commentCollasepBtn__3OMXE')
            if expand_button:
                print(f"å‘ç°å±•å¼€æŒ‰é’®ï¼Œæ­£åœ¨å°è¯•ç‚¹å‡»å±•å¼€ç¬¬ {index + 1} æ¡è¯„è®º")
                # æ¨¡æ‹Ÿç‚¹å‡»å±•å¼€æŒ‰é’®
                expand_button_element = driver.find_element(By.XPATH, f'//div[@id="comment{comment_div["id"].split("comment")[-1]}"]//div[contains(@class, "collapse-text_expandBtn__R7Ga3")]')
                driver.execute_script("arguments[0].click();", expand_button_element)
                time.sleep(2)  # å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œç¡®ä¿è¯„è®ºå†…å®¹å®Œå…¨å±•å¼€
                print(f"æˆåŠŸå±•å¼€ç¬¬ {index + 1} æ¡è¯„è®º")
                discussion_page_source = driver.page_source
                discussion_soup = BeautifulSoup(discussion_page_source, 'html.parser')
                # é‡æ–°è·å–å½“å‰è¯„è®ºçš„ div
                comment_div = discussion_soup.select_one(f'div[id="{comment_div["id"]}"]')

            # è·å–è¯„è®ºäººå¤´åƒå’Œç”¨æˆ·ID
            avatar_box = comment_div.find('div', class_='comments_avatarBox__7xweF')
            avatar_img = avatar_box.find('img', class_='comments_avatar__FU5C5')
            avatar_url = avatar_img['src'] if avatar_img else None
            user_id = re.search(r'passport_(\d+)', avatar_url).group(1) if avatar_url else None

            # è·å–è¯„è®ºå†…å®¹
            nickname_element = comment_div.find('div', class_=re.compile(r'^comments_name__VQiPd'))
            nickname = nickname_element.get_text(strip=True)

            comment_time_element = comment_div.find('div', class_='comments_time__00lty')
            comment_time = comment_time_element.get_text(strip=True)

            comment_content_element = comment_div.find('div', class_='collapse-text_collapseText__zVrd2 comments_commentText__D48oR')
            comment_content = comment_content_element.get_text(strip=True)

            like_count_element = comment_div.find('span', id='text')
            like_count = like_count_element.get_text(strip=True)

            if nickname and comment_time and comment_content and like_count:
                comments.append({
                    "nickname": nickname,
                    "user_id": user_id,
                    "avatar_url": avatar_url,
                    "comment_time": comment_time,
                    "comment_content": comment_content,
                    "like_count": like_count
                })
        except Exception as e:
            print(f"\"{video_name}\"åœ¨æå–ç¬¬ {index + 1} æ¡è¯„è®ºä¿¡æ¯æ—¶å‡ºé”™ï¼š{e}")
            continue

    return comments

def save_to_csv(details):
    """
    å°†è§†é¢‘è¯¦æƒ…ä¿¡æ¯ä¿å­˜åˆ° CSV æ–‡ä»¶
    :param details: è§†é¢‘è¯¦æƒ…ä¿¡æ¯åˆ—è¡¨
    """
    try:
        with file_lock:
            # åºåˆ—åŒ–è¯„è®ºæ•°æ®
            for detail in details:
                if 'comments' in detail:
                    try:
                        detail['comments'] = json.dumps(detail['comments'], ensure_ascii=False)
                    except Exception as e:
                        logging.error(f"åºåˆ—åŒ–å¤±è´¥è¯„è®ºæ•°æ®ï¼š{detail['comments']}")
                        raise

            details_df = pd.DataFrame(details)
            
            # è·å–å½“å‰æ—¶é—´å¹¶æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
            current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # æŒ‡å®šç›¸å¯¹è·¯å¾„ï¼Œæ–‡ä»¶åä¸­åŒ…å«å½“å‰æ—¶é—´
            save_path = os.path.join("data/hotdata", f"iqiyi_{current_time}.csv")
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # ä¿å­˜ DataFrame åˆ° CSV æ–‡ä»¶
            details_df.to_csv(save_path, index=False, encoding="utf-8-sig")
            logging.info(f"è§†é¢‘è¯¦æƒ…ä¿¡æ¯å·²ä¿å­˜åˆ° {save_path} æ–‡ä»¶ä¸­ã€‚")
    except Exception as e:
        logging.error(f"ä¿å­˜CSVæ–‡ä»¶æ—¶å‡ºé”™ï¼š{e}", exc_info=True)
        return False
    return True

# å…¨å±€ä¼šè¯å¯¹è±¡ï¼Œæå‡è¿æ¥æ± å®¹é‡ï¼Œå‡å°‘è¿æ¥è¢«ä¸¢å¼ƒ
session = requests.Session()
session.mount('http://', requests.adapters.HTTPAdapter(pool_connections=20, pool_maxsize=20))
session.mount('https://', requests.adapters.HTTPAdapter(pool_connections=20, pool_maxsize=20))